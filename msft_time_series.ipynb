# Refit on all data and forecast next 60 business days
full_series = series
full_model = ARIMA(full_series, order=order).fit()
future_index = pd.bdate_range(start=full_series.index[-1] + pd.offsets.BDay(), periods=60)
future_forecast = pd.Series(full_model.forecast(steps=len(future_index)), index=future_index)

plt.figure(figsize=(12,6))
plt.plot(full_series.index[-400:], full_series.iloc[-400:], label='History')
plt.plot(future_index, future_forecast, label='Forecast', color='red')
plt.title('Next 60 Business Days Forecast')
plt.legend()
plt.show()

future_forecast.head()

from statsmodels.tsa.arima.model import ARIMA

order = auto_model.order
print("Selected order:", order)

sm_model = ARIMA(train, order=order)
sm_res = sm_model.fit()
print(sm_res.summary())

sm_pred = sm_res.forecast(steps=len(test))
sm_pred = pd.Series(sm_pred, index=test.index)

rmse = mean_squared_error(test, sm_pred, squared=False)
mae = mean_absolute_error(test, sm_pred)
print(f"Statsmodels ARIMA{order}: RMSE={rmse:.3f}, MAE={mae:.3f}")
import pmdarima as pm

# Use auto_arima to select (p,d,q)
auto_model = pm.auto_arima(
    train,
    start_p=0, start_q=0,
    max_p=5, max_q=5,
    d=None, seasonal=False,
    test='adf',
    trace=False,
    error_action='ignore',
    suppress_warnings=True,
    stepwise=True,
)
print(auto_model.summary())

# Fit final model on train
auto_model.fit(train)
auto_pred = pd.Series(auto_model.predict(n_periods=len(test)), index=test.index)

rmse = mean_squared_error(test, auto_pred, squared=False)
mae = mean_absolute_error(test, auto_pred)
print(f"Auto-ARIMA: RMSE={rmse:.3f}, MAE={mae:.3f}")
from sklearn.metrics import mean_absolute_error, mean_squared_error

def train_test_split_series(s: pd.Series, test_size: int = 252):
    s = s.dropna().astype(float)
    return s.iloc[:-test_size], s.iloc[-test_size:]

train, test = train_test_split_series(series, test_size=252)
print(train.index.min(), "->", train.index.max(), "|", test.index.min(), "->", test.index.max())

# Baselines
naive_forecast = pd.Series(train.iloc[-1], index=test.index)
ma_forecast = pd.Series(train.rolling(window=20).mean().iloc[-1], index=test.index)

for name, pred in {"Naive": naive_forecast, "MA20": ma_forecast}.items():
    rmse = mean_squared_error(test, pred, squared=False)
    mae = mean_absolute_error(test, pred)
    print(f"{name}: RMSE={rmse:.3f}, MAE={mae:.3f}")
from statsmodels.tsa.stattools import adfuller

series = df['Adj Close'] if 'Adj Close' in df.columns else df['Close']
series = series.dropna().astype(float)

adf_stat, pvalue, *_ = adfuller(series)
print(f"ADF: {adf_stat:.3f}, p-value: {pvalue:.3g}")
fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)
if 'Adj Close' in df.columns:
    axes[0].plot(df.index, df['Adj Close'], label='Adj Close')
else:
    axes[0].plot(df.index, df['Close'], label='Close')
axes[0].set_title('MSFT Price')
axes[0].legend()

if 'Volume' in df.columns:
    axes[1].plot(df.index, df['Volume'], color='gray', label='Volume')
    axes[1].legend()
plt.tight_layout()
plt.show()

sns.histplot(df['Close'].dropna(), kde=True)
plt.title('Close Distribution')
plt.show()
df_raw = pd.read_csv(LOCAL_CSV)
# Normalize column names
cols = {c: c.strip().replace("Adj_Close", "Adj Close").title() for c in df_raw.columns}
df_raw = df_raw.rename(columns=cols)

# Parse date
date_col = 'Date' if 'Date' in df_raw.columns else next((c for c in df_raw.columns if 'date' in c.lower()), None)
assert date_col is not None, "Date column not found"
df_raw[date_col] = pd.to_datetime(df_raw[date_col])
df_raw = df_raw.set_index(date_col).sort_index()

# Keep standard OHLCV columns if present
keep = [c for c in ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'] if c in df_raw.columns]
df = df_raw[keep].copy()

# Business day frequency
df = df.asfreq('B')

# Forward fill missing prices, fill volume with 0 if absent
for c in [c for c in ['Open','High','Low','Close','Adj Close'] if c in df.columns]:
    df[c] = df[c].ffill()
if 'Volume' in df.columns:
    df['Volume'] = df['Volume'].fillna(0)

print(df.tail())
print(df.isna().sum())
if not LOCAL_CSV.exists():
    got = False
    if KAGGLE_DATASET and KAGGLE_FILE:
        got = try_download_from_kaggle(KAGGLE_DATASET, KAGGLE_FILE, LOCAL_CSV)
    if not got:
        got = try_download_from_yfinance("MSFT", LOCAL_CSV)
    print("Data present:", got, "at", LOCAL_CSV)
else:
    print("Found local CSV:", LOCAL_CSV)
def try_download_from_yfinance(ticker: str, dest_path: Path) -> bool:
    try:
        import yfinance as yf
        print(f"Downloading {ticker} from yfinance...")
        df = yf.download(ticker, period="max", interval="1d", auto_adjust=False)
        if df.empty:
            print("yfinance returned empty frame")
            return False
        df = df.rename(columns={"Adj Close": "Adj Close"})
        df.index.name = "Date"
        df.to_csv(dest_path)
        print("yfinance download complete.")
        return True
    except Exception as e:
        print("yfinance download failed:", e)
        return False
def try_download_from_kaggle(dataset: str, file_name: str, dest_path: Path) -> bool:
    if not dataset or not file_name:
        return False
    if not kaggle_available:
        return False
    try:
        # Ensure ~/.kaggle/kaggle.json exists with 600 perms
        print(f"Attempting Kaggle download: {dataset} -> {file_name}")
        import subprocess
        cmd = [
            sys.executable, "-m", "kaggle", "datasets", "download",
            "-d", dataset, "-f", file_name, "-p", str(DATA_DIR), "--force"
        ]
        subprocess.check_call(cmd)
        # If zipped, unzip
        zip_path = DATA_DIR / (file_name + ".zip")
        if zip_path.exists():
            import zipfile
            with zipfile.ZipFile(zip_path, 'r') as zf:
                zf.extractall(DATA_DIR)
            zip_path.unlink(missing_ok=True)
        # Move/rename to dest
        candidate = DATA_DIR / file_name
        if candidate.exists():
            candidate.rename(dest_path)
        print("Kaggle download complete.")
        return dest_path.exists()
    except Exception as e:
        print("Kaggle download failed:", e)
        return False
from typing import Optional

KAGGLE_DATASET = os.environ.get("MSFT_KAGGLE_DATASET", "")  # e.g., "username/microsoft-stock-time-series-analysis"
KAGGLE_FILE = os.environ.get("MSFT_KAGGLE_FILE", "")        # e.g., "microsoft_stock.csv"
LOCAL_CSV = DATA_DIR / "microsoft_stock.csv"

try:
    import kaggle  # noqa: F401
    kaggle_available = True
except Exception:
    kaggle_available = False

print("Kaggle available:", kaggle_available)
print("KAGGLE_DATASET:", KAGGLE_DATASET)
print("KAGGLE_FILE:", KAGGLE_FILE)
print("Local CSV path:", LOCAL_CSV)
### Data acquisition

This notebook tries to download from Kaggle first. If Kaggle isn't configured, it falls back to downloading MSFT OHLC data via yfinance.

- Kaggle dataset slug to use: set `KAGGLE_DATASET` below. If unknown, manually place your CSV under `/workspace/data/msft/`.
- Expected CSV columns: `Date`, `Open`, `High`, `Low`, `Close`, `Adj Close` (or `Adj_Close`), `Volume`.
import os
import sys
import warnings

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from pathlib import Path
from datetime import timedelta

warnings.filterwarnings("ignore")
plt.style.use("seaborn-v0_8")

DATA_DIR = Path("/workspace/data/msft")
DATA_DIR.mkdir(parents=True, exist_ok=True)

print(f"Python {sys.version}")
print("Data dir:", DATA_DIR)
## Microsoft Stock (MSFT) Time Series Analysis and Forecasting

This notebook loads the Kaggle dataset "Microsoft Stock- Time Series Analysis" (or a local CSV), performs exploratory analysis, prepares the data, and trains forecasting models:
- Auto ARIMA (pmdarima)
- Statsmodels ARIMA
- Baselines (naive last value, moving average)

It outputs evaluation metrics and a forecast plot.
